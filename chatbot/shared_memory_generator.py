# -*- coding: utf-8 -*-
"""
å…±åŒå›æ†¶æ•´ç†å·¥å…·ï¼ˆCLI ç‰ˆï¼Œç„¡ GUIï¼‰
- è®€å–ä¸€å€‹ {è§’è‰²å}.txtï¼ˆæ¯è¡Œ = ä¸€æ®µå…±åŒå›æ†¶ï¼‰
- ä»¥ OpenAI GPT é€æ®µæ¿ƒç¸®ï¼ˆè‹¥æœ‰ API keyï¼‰ï¼Œä¿ç•™æ—¥æœŸã€ä¸é‡è¤‡
- è¿½åŠ å¯«å…¥åˆ° shared_memories/shared_memories_{è§’è‰²å}.txt
- ä¹Ÿæä¾› process_file(input_path) ä¾›ä»–ç³»çµ±ç¨‹å¼åŒ–å‘¼å«
"""
import os
import re
import json
import argparse
from dataclasses import dataclass
from typing import List, Tuple
from pathlib import Path
from datetime import datetime

# ===================== è¨­å®š =====================
CONFIG_PATH = os.getenv("CONFIG_PATH", "config.json")
CONFIG = {}
if os.path.exists(CONFIG_PATH):
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        CONFIG = json.load(f)

MODEL_NAME = CONFIG.get("model_name", "gpt-4.1-nano")
OPENAI_API_KEY = (CONFIG.get("openai_api_key") or "").strip() if CONFIG else ""
AVOID_DUP_IN_FILE = bool(CONFIG.get("avoid_duplicates_in_file", True)) if CONFIG else True

BASE_DIR = Path(__file__).resolve().parent
SHARED_DIR = BASE_DIR / "shared_memories"
SHARED_DIR.mkdir(parents=True, exist_ok=True)

# ===================== OpenAIï¼ˆå¯é¸ï¼‰ =====================
USE_OPENAI = False
try:
    import openai  # pip install openai
    if OPENAI_API_KEY:
        openai.api_key = OPENAI_API_KEY
        USE_OPENAI = True
except Exception:
    USE_OPENAI = False

# ===================== è³‡æ–™æ¨¡å‹ =====================
@dataclass
class MemoryItem:
    summary: str
    detail: str

# ===================== æ­£å‰‡ï¼ˆæ—¥æœŸ/æ™‚é–“ï¼‰ =====================
RE_DATE_YMD = re.compile(r"\b\d{4}[-/]\d{1,2}[-/]\d{1,2}\b")  # 2025-08-30 / 2025/08/30
RE_DATE_MD  = re.compile(r"\b\d{1,2}/\d{1,2}\b")               # 8/30
RE_DATE_CJK = re.compile(r"\b\d{1,2}æœˆ\d{1,2}æ—¥\b")            # 8æœˆ30æ—¥
RE_TIME_HM  = re.compile(r"^\s*\d{1,2}:\d{2}\s+")              # è¡Œé¦– HH:MM

def extract_dates(s: str) -> List[str]:
    dates = set()
    for pat in (RE_DATE_YMD, RE_DATE_MD, RE_DATE_CJK):
        for m in pat.finditer(s):
            dates.add(m.group(0))
    return sorted(dates)

# ===================== å·¥å…· =====================
def role_from_filename(path: Path) -> str:
    """
    ç”±è¼¸å…¥æª”åå–å¾—è§’è‰²åï¼š
      åª½å’ªğŸ«¶.txt â†’ åª½å’ªğŸ«¶
      [LINE]åª½å’ªğŸ«¶.txt â†’ åª½å’ªğŸ«¶
    """
    stem = path.stem
    stem = re.sub(r"^\s*\[.*?\]\s*", "", stem).strip()
    return stem or "default"

def get_output_path(role: str) -> Path:
    safe_role = re.sub(r"[\\/]+", "_", role.strip() or "default")
    return SHARED_DIR / f"shared_memories_{safe_role}.txt"

def normalize_key(s: str) -> str:
    return re.sub(r"\s+", "", (s or "").lower())

def deduplicate_items(items: List[MemoryItem]) -> List[MemoryItem]:
    seen, out = set(), []
    for it in items:
        key = normalize_key(it.detail)
        if key in seen:
            continue
        seen.add(key)
        out.append(it)
    return out

def filter_out_existing(path: Path, items: List[MemoryItem]) -> List[MemoryItem]:
    if not (AVOID_DUP_IN_FILE and path.exists()):
        return items
    existed = set()
    try:
        with path.open("r", encoding="utf-8") as fr:
            for line in fr:
                parts = line.rstrip("\n").split("\t", 1)
                if len(parts) == 2:
                    existed.add(normalize_key(parts[1]))  # ä»¥ detail å»é‡
    except Exception:
        pass
    return [it for it in items if normalize_key(it.detail) not in existed]

# ===================== LLM æ¿ƒç¸®ï¼ˆé€æ®µï¼Œä¿ç•™æ—¥æœŸï¼‰ =====================
PROMPT_SYS = (
    "ä½ æ˜¯å…±åŒå›æ†¶æ•´ç†åŠ©æ‰‹ã€‚ä»¥ä¸‹è¼¸å…¥ç‚ºå¤šæ®µã€ç²—ç•¥å…±åŒå›æ†¶ã€ï¼Œæ¯è¡Œä¸€æ®µã€‚\n"
    "è«‹é€æ®µæ•´ç†é‡é»ï¼Œè¼¸å‡º JSON é™£åˆ—ï¼ˆè‹¥å…§å®¹æ¥µçŸ­/ç„¡æ„ç¾©å¯è·³éï¼Œä¸è¦åˆä½µä¸åŒæ®µè½ï¼‰ã€‚\n"
    "è¦å‰‡ï¼š\n"
    "1) è‹¥åŸå¥å«ã€æ—¥æœŸã€(YYYY/MM/DDã€YYYY-MM-DDã€MM/DDã€MæœˆDæ—¥)ï¼Œè«‹åŸæ¨£ä¿ç•™åœ¨ summary èˆ‡ detail ä¸­ï¼›\n"
    "2) ç§»é™¤è¡Œé¦–ã€æ™‚é–“ HH:MMã€èˆ‡ç¨±å‘¼/åå­—ç­‰å™ªéŸ³ï¼ˆè‹¥å¯è¾¨è­˜ï¼‰ï¼Œä½†ä¸è¦æ”¹å‹•æ—¥æœŸï¼›\n"
    "3) ä¸è¦ç™¼æ˜ä¸å­˜åœ¨çš„æ—¥æœŸèˆ‡ç´°ç¯€ï¼›\n"
    "4) æ¯é …ï¼š{ \"summary\": \"20~40å­—æ‘˜è¦ï¼ˆè‹¥æœ‰æ—¥æœŸå¯åŒ…å«ï¼‰\", \"detail\": \"1~2å¥å®Œæ•´æè¿°ï¼ˆå‹™å¿…ä¿ç•™åŸå¥æ—¥æœŸï¼‰\" }ï¼›\n"
    "5) åƒ…è¼¸å‡º JSONï¼Œå‹¿åŠ ä»»ä½•è§£èªªã€‚"
)

def _safe_json_block(s: str) -> str:
    s = (s or "").replace("\r", " ").replace("\n", " ").strip()
    m = re.search(r"\[[\s\S]*\]|\{[\s\S]*\}", s)
    return m.group(0) if m else "[]"

def llm_summarize_lines(lines: List[str]) -> List[MemoryItem]:
    if not USE_OPENAI:
        # æœ¬åœ°ä¿åº•ï¼šå»æ‰è¡Œé¦– HH:MMï¼›summary å–å‰ 40 å­—
        items = []
        for raw in lines:
            txt = raw.strip()
            if not txt:
                continue
            txt = RE_TIME_HM.sub("", txt)
            summ = txt[:40] + ("â€¦" if len(txt) > 40 else "")
            items.append(MemoryItem(summary=summ, detail=txt))
        return deduplicate_items(items)

    try:
        payload = "\n".join(lines)
        messages = [
            {"role": "system", "content": PROMPT_SYS},
            {"role": "user",   "content": f"è«‹æ•´ç†ä¸‹åˆ—å¤šæ®µå…±åŒå›æ†¶ï¼ˆæ¯è¡Œä¸€æ®µï¼‰ï¼š\n{payload}"}
        ]
        resp = openai.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=0.2,
        )
        content = _safe_json_block(resp.choices[0].message.content)
        data = json.loads(content)
        items: List[MemoryItem] = []
        for obj in data:
            summ = (obj.get("summary") or "").strip()
            det  = (obj.get("detail") or "").strip()
            if not summ or not det:
                continue
            # è‹¥ detail ç„¡æ—¥æœŸï¼Œä½†åŸè¼¸å…¥æ•´é«”æœ‰æ—¥æœŸ â†’ è£œæ•‘ï¼ˆå–æœ€å¸¸è¦‹çš„ç¬¬ä¸€å€‹ï¼‰
            if not (RE_DATE_YMD.search(det) or RE_DATE_MD.search(det) or RE_DATE_CJK.search(det)):
                all_dates = []
                for ln in lines:
                    all_dates += extract_dates(ln)
                if all_dates:
                    det  = f"{all_dates[0]} {det}"
                    summ = f"{all_dates[0]} " + summ
            items.append(MemoryItem(
                summary=summ[:40] + ("â€¦" if len(summ) > 40 else ""),
                detail=det
            ))
        return deduplicate_items(items)
    except Exception as e:
        print("[WARN] OpenAI æ¿ƒç¸®å¤±æ•—ï¼Œæ”¹ç”¨æœ¬åœ°ä¿åº•ï¼š", e)
        # fallback
        items = []
        for raw in lines:
            txt = raw.strip()
            if not txt:
                continue
            txt = RE_TIME_HM.sub("", txt)
            summ = txt[:40] + ("â€¦" if len(txt) > 40 else "")
            items.append(MemoryItem(summary=summ, detail=txt))
        return deduplicate_items(items)

# ===================== å¯«æª” =====================
def append_shared_memories(role: str, memories: List[MemoryItem]) -> Tuple[Path, int]:
    out_path = get_output_path(role)

    # éæ¿¾å·²å­˜åœ¨çš„é‡è¤‡
    memories = filter_out_existing(out_path, memories)

    # ç¢ºä¿æª”å°¾æ›è¡Œ
    needs_nl = False
    if out_path.exists():
        try:
            with out_path.open("rb") as frb:
                frb.seek(max(frb.seek(0, os.SEEK_END) or 0 - 1, 0))
                last = frb.read() or b""
                needs_nl = (not last.endswith(b"\n"))
        except Exception:
            needs_nl = False

    cnt = 0
    with out_path.open("a", encoding="utf-8") as fw:
        if needs_nl:
            fw.write("\n")
        for m in memories:
            summary = m.summary.replace("\t", " ").strip()
            detail  = m.detail.replace("\t", " ").strip()
            if not summary or not detail:
                continue
            fw.write(f"{summary}\t{detail}\n")
            cnt += 1
    return out_path, cnt

# ===================== æ ¸å¿ƒæµç¨‹ =====================
def process_file(input_path: str) -> Tuple[str, int, List[MemoryItem]]:
    """
    è®€å– {è§’è‰²å}.txt â†’ æ¯è¡Œä¸€æ®µ â†’ æ¿ƒç¸® â†’ è¿½åŠ å¯«å…¥ shared_memories/shared_memories_{è§’è‰²å}.txt
    å›å‚³ï¼š(è¼¸å‡ºæª”è·¯å¾‘, å¯«å…¥ç­†æ•¸, å¯«å…¥å…§å®¹åˆ—è¡¨)
    """
    path = Path(input_path)
    if not path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æª”ï¼š{path}")

    role = role_from_filename(path)

    # è®€è¡Œï¼ˆå¤šç·¨ç¢¼å˜—è©¦ï¼‰
    raw = path.read_bytes()
    text = None
    for enc in ("utf-8", "utf-8-sig", "cp950", "big5", "utf-16"):
        try:
            text = raw.decode(enc)
            break
        except Exception:
            continue
    if text is None:
        text = raw.decode("utf-8", errors="ignore")

    # ä¾æ›è¡Œåˆ†æ®µ
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    if not lines:
        return str(get_output_path(role)), 0, []

    # LLM æ¿ƒç¸® / æœ¬åœ°ä¿åº•
    items = llm_summarize_lines(lines)
    if not items:
        return str(get_output_path(role)), 0, []

    # å¯«å…¥
    out_path, n = append_shared_memories(role, items)
    return str(out_path), n, items

# ===================== CLI =====================
def main():
    # å…ˆå®£å‘Š globalï¼Œé¿å… "used prior to global declaration"
    global MODEL_NAME

    import argparse
    parser = argparse.ArgumentParser(description="å…±åŒå›æ†¶æ•´ç†ï¼ˆCLI ç‰ˆï¼‰")

    # æ”¹æˆæ——æ¨™å¼è¼¸å…¥ï¼Œç¬¦åˆä½ çš„å‘¼å«æ–¹å¼
    parser.add_argument("--input", "-i", required=True,
                        help="{è§’è‰²å}.txt æª”æ¡ˆè·¯å¾‘ï¼ˆæ¯è¡Œä¸€æ®µå…±åŒå›æ†¶ï¼‰")

    # default æ”¹æˆ Noneï¼Œé¿å…åœ¨å®£å‘Š global å‰å¼•ç”¨ MODEL_NAME
    parser.add_argument("--model", default=None,
                        help="OpenAI æ¨¡å‹åç¨±ï¼ˆè‹¥ä¸æŒ‡å®šå‰‡ä½¿ç”¨ config.json çš„ model_nameï¼‰")

    args = parser.parse_args()

    # è‹¥æœ‰æŒ‡å®š --modelï¼Œæ‰è¦†å¯«å…¨åŸŸ MODEL_NAME
    if args.model:
        MODEL_NAME = args.model

    print(f"[INFO] OpenAI={'ON' if USE_OPENAI else 'OFF'} | Model={MODEL_NAME}")
    out_path, n, items = process_file(args.input)

    print(f"[OK] å·²å¯«å…¥ï¼š{out_path}ï¼Œå…± {n} ç­†")
    for it in items:
        print(f"- {it.summary}\t{it.detail}")


if __name__ == "__main__":
    main()
