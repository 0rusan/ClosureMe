# shared_memory.py
import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from openai import OpenAI  # âœ… æ–°ç‰ˆ openai å¥—ä»¶åˆå§‹åŒ–


def _load_config(config_path: str = "config.json") -> dict:
    """
    å˜—è©¦è¼‰å…¥ config.jsonã€‚è‹¥ä¸å­˜åœ¨å‰‡å›å‚³ç©º dictã€‚
    æ”¯æ´å…©ç¨®å¯«æ³•ï¼š
      1) å¹³é‹ªï¼š
         {
           "openai_api_key": "...",
           "shared_memory_base_dir": "...",
           "shared_memory_embedding_model": "...",
           "shared_memory_embedding_dim": 384
         }
      2) å€å¡Šï¼š
         {
           "openai_api_key": "...",
           "shared_memory": {
             "base_dir": "...",
             "embedding_model": "...",
             "embedding_dim": 384
           }
         }
    """
    try:
        with open(config_path, "r", encoding="utf-8") as f:
            cfg = json.load(f)
        return cfg if isinstance(cfg, dict) else {}
    except Exception:
        return {}


class SharedMemoryManager:
    def __init__(
        self,
        character="default",
        config_path: str = "config.json",
        # â¬‡ï¸ ä»¥ä¸‹åƒæ•¸è‹¥ä¸æä¾›ï¼Œæœƒå¾ config.json è®€ï¼›è‹¥æä¾›ï¼Œå„ªå…ˆä½¿ç”¨åƒæ•¸å€¼
        base_dir: str | None = None,
        model_name: str | None = None,
        embedding_dim: int | None = None,
        openai_key: str | None = None,
    ):
        """
        åˆå§‹åŒ–è¦å‰‡ï¼ˆå„ªå…ˆé †åºï¼‰ï¼šå‡½å¼åƒæ•¸ > config.json > é è¨­å€¼
        """
        cfg = _load_config(config_path)

        # è®€å– configï¼ˆåŒæ™‚æ”¯æ´å¹³é‹ªèˆ‡åµŒå¥—çµæ§‹ï¼‰
        # --- OpenAI é‡‘é‘° ---
        cfg_openai_key = cfg.get("openai_api_key") or (cfg.get("openai") or {}).get("api_key")

        # --- Shared Memory å€å¡Š ---
        nested = cfg.get("shared_memory") or {}
        cfg_base_dir = (
            base_dir
            or cfg.get("shared_memory_base_dir")
            or nested.get("base_dir")
            or "shared_memories"
        )
        cfg_model_name = (
            model_name
            or cfg.get("shared_memory_embedding_model")
            or nested.get("embedding_model")
            or "paraphrase-multilingual-MiniLM-L12-v2"
        )
        cfg_embedding_dim = int(
            embedding_dim
            or cfg.get("shared_memory_embedding_dim")
            or nested.get("embedding_dim")
            or 384
        )

        # æœ€çµ‚è¨­å®š
        self.character = character
        self.embedding_dim = cfg_embedding_dim
        self.base_dir = cfg_base_dir
        self.memory_file = os.path.join(self.base_dir, f"shared_memories_{character}.txt")
        self.index_file = os.path.join(self.base_dir, f"shared_faiss_{character}.idx")
        self.openai_key = openai_key or cfg_openai_key or os.getenv("OPENAI_API_KEY")

        os.makedirs(self.base_dir, exist_ok=True)

        # å…§éƒ¨ç‹€æ…‹
        self.summaries = []     # summary list
        self.full_texts = []    # detailed list
        self.index = faiss.IndexFlatL2(self.embedding_dim)

        # å‘é‡æ¨¡å‹
        self.model = SentenceTransformer(cfg_model_name)

        # è¼‰å…¥æ—¢æœ‰è¨˜æ†¶
        if self.model:
            self._load_memories()

    # ------------------ æª”æ¡ˆè¼‰å…¥/å„²å­˜ ------------------
    def _load_memories(self):
        if not os.path.exists(self.memory_file):
            print("â„¹ï¸ ç„¡å…±åŒè¨˜æ†¶æª”æ¡ˆï¼Œåˆå§‹åŒ–ç©ºè¨˜æ†¶åº«")
            return

        with open(self.memory_file, 'r', encoding='utf-8') as f:
            lines = [line.strip() for line in f if line.strip()]

        for line in lines:
            if '\t' in line:
                summary, detail = line.split('\t', 1)
                self.summaries.append(summary)
                self.full_texts.append(detail)

        if self.summaries:
            embeddings = self.model.encode(
                self.summaries,
                convert_to_numpy=True,
                normalize_embeddings=True
            ).astype('float32')
            self.index.add(embeddings)
            print(f"âœ… è¼‰å…¥ {len(self.summaries)} ç­†å…±åŒå›æ†¶")

    def add_memory(self, summary, full_text):
        if summary in self.summaries:
            print("âš ï¸ è©²å›æ†¶å·²å­˜åœ¨ï¼Œç•¥é")
            return
        self.summaries.append(summary)
        self.full_texts.append(full_text)
        embedding = self.model.encode(
            [summary], convert_to_numpy=True,
            normalize_embeddings=True
        ).astype('float32')
        self.index.add(embedding)
        with open(self.memory_file, 'a', encoding='utf-8') as f:
            f.write(f"{summary}\t{full_text}\n")
        print(f"ğŸ§  æ–°å¢å…±åŒå›æ†¶ï¼š{summary}")

    # ------------------ æª¢ç´¢ ------------------
    def search_memories(self, query, k=3):
        if not self.summaries:
            return [], None, []
        embedding = self.model.encode(
            [query], convert_to_numpy=True,
            normalize_embeddings=True
        ).astype('float32')
        distances, indices = self.index.search(embedding, min(k, len(self.summaries)))
        results = [
            {"brief": self.summaries[i], "detail": self.full_texts[i], "distance": float(d)}
            for i, d in zip(indices[0], distances[0]) if i < len(self.summaries)
        ]
        return results, embedding, distances[0]

    # ------------------ è‡ªå‹•æ‘˜è¦ï¼ˆé€é OpenAIï¼‰ ------------------
    def auto_extract_shared_memory(self, user_input, ai_response, openai_api_key=None):
        """
        ä½¿ç”¨ OpenAI ç”¢ç”Ÿã€Œç°¡è¦æ‘˜è¦ / è©³ç´°å…§å®¹ã€ï¼ŒæˆåŠŸå¾Œè‡ªå‹•å¯«å…¥è¨˜æ†¶æª”ã€‚
        é‡‘é‘°å„ªå…ˆé †åºï¼šå‚³å…¥åƒæ•¸ > config.json > ç’°å¢ƒè®Šæ•¸ OPENAI_API_KEY
        """
        key = openai_api_key or self.openai_key or os.getenv("OPENAI_API_KEY")
        if not key:
            print("âŒ æ‰¾ä¸åˆ°å¯ç”¨çš„ OpenAI API é‡‘é‘°")
            return None, None

        client = OpenAI(api_key=key)

        prompt = f"""ä½ æ˜¯ä¸€å€‹ç¸½çµåŠ©æ‰‹ã€‚è«‹å°‡ä»¥ä¸‹å°è©±å…§å®¹æ•´ç†æˆä¸€æ®µã€Œå…±åŒå›æ†¶æ‘˜è¦ã€ï¼Œä»¥ä¾›AIè¨˜éŒ„ã€‚è¼¸å‡ºæ ¼å¼å¦‚ä¸‹ï¼š
ç°¡è¦æ‘˜è¦ï¼šxxx
è©³ç´°å…§å®¹ï¼šyyy

å°è©±å¦‚ä¸‹ï¼š
ä½¿ç”¨è€…èªªï¼šã€Œ{user_input}ã€
AI å›è¦†ï¼šã€Œ{ai_response}ã€

è«‹è¼¸å‡ºçµæœï¼š"""

        try:
            response = client.chat.completions.create(
                model="gpt-4.1-nano",
                messages=[{"role": "user", "content": prompt}]
            )
            text = response.choices[0].message.content.strip()
            summary, detail = "", ""
            for line in text.splitlines():
                if "ç°¡è¦æ‘˜è¦" in line:
                    summary = line.split("ï¼š", 1)[-1].strip()
                elif "è©³ç´°å…§å®¹" in line:
                    detail = line.split("ï¼š", 1)[-1].strip()
            if summary and detail:
                self.add_memory(summary, detail)
                return summary, detail
        except Exception as e:
            print("âŒ è‡ªå‹•æå–å›æ†¶å¤±æ•—ï¼š", e)
        return None, None
